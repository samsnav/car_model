{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "6db682c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd \n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "34aa3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and analyze features\n",
    "training = pd.read_csv('Training_DataSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "303550a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check to see if any listing prices or vehicle trim are missing \n",
    "empty_cells_in_price = training['Dealer_Listing_Price'].isna() | (training['Dealer_Listing_Price'] == '')\n",
    "empty_cells_in_trim = training['Vehicle_Trim'].isna() | (training['Vehicle_Trim'] == '')\n",
    "\n",
    "## Drop rows with empty listing prices and trim as we cant learn from these\n",
    "rows_to_drop = empty_cells_in_price | empty_cells_in_trim\n",
    "training = training[~rows_to_drop].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "3015b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "## City change cities to high or low population\n",
    "## These cities are expensive and probably have higher car prices\n",
    "top_16_us_cities_by_population = [\n",
    "    \"New York City\",\n",
    "    \"Los Angeles\",\n",
    "    \"Chicago\",\n",
    "    \"Houston\",\n",
    "    \"Phoenix\",\n",
    "    \"Philadelphia\",\n",
    "    \"San Antonio\",\n",
    "    \"San Diego\",\n",
    "    \"Dallas\",\n",
    "    \"San Jose\",\n",
    "    \"Austin\",\n",
    "    \"Jacksonville\",\n",
    "    \"Fort Worth\",\n",
    "    \"Columbus\",\n",
    "    \"Charlotte\",\n",
    "    \"Indianapolis\"\n",
    "]\n",
    "training['high_population'] = training['SellerCity'].apply(lambda x: 1 if x in top_16_us_cities_by_population else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "ac223c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with seller information\n",
    "## We will only keep state and a high low city population indicator to avoid overfitting\n",
    "training = training.drop(columns=['SellerCity', 'SellerZip'])\n",
    "\n",
    "##Convert states into booleans which will allow XGboost to work with these features.\n",
    "training = pd.get_dummies(training, columns=['SellerState'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "3ce01fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SellerListSrc\n",
       "Digital Motorworks (DMi)    3043\n",
       "Inventory Command Center    1180\n",
       "HomeNet Automotive          1029\n",
       "Jeep Certified Program       544\n",
       "My Dealer Center              29\n",
       "Sell It Yourself              14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Seller List SRC\n",
    "training['SellerListSrc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "fcaadb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check SellerListSrc for NA and replace values then convert to dummy variables\n",
    "training.loc[training['SellerListSrc'].isna()]\n",
    "training['SellerListSrc'] = training['SellerListSrc'].fillna('Sell It Yourself')\n",
    "training = pd.get_dummies(training, columns=['SellerListSrc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "facde6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SellerIsPriv\n",
    "## Only 14 Trues this will not provide that much information and potentially cause overfitting\n",
    "## remove this column \n",
    "training['SellerIsPriv'].sum()\n",
    "training = training.drop(columns=['SellerIsPriv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "d0d760da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SellerName\n",
    "## Most of the names have the type of the car in it\n",
    "## We can delete this as this information is contained in VehMake\n",
    "training['SellerName'].head()\n",
    "training = training.drop(columns=['SellerName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "1f7db6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Vehicle Information\n",
    "## VehBodyStyle \n",
    "## only SUV will not play a factor remove\n",
    "training['VehBodystyle'].value_counts()\n",
    "training = training.drop(columns=['VehBodystyle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "b5a8ab5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VehCertified\n",
       "False    4820\n",
       "True     1021\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Vehicle certified\n",
    "##Certified already in true and false form \n",
    "training['VehCertified'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "b0bc33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Vehicle Exterior, interior\n",
    "training['VehColorExt'].value_counts()\n",
    "training['VehColorInt'].value_counts()\n",
    "\n",
    "training['VehColorExt'].fillna('Unknown', inplace=True)\n",
    "training['VehColorInt'].fillna('Unknown', inplace=True)\n",
    "\n",
    "#Convert to simpler color groups\n",
    "color_groups = {\n",
    "    'Black': ['black'],\n",
    "    'White': ['white'],\n",
    "    'Silver': ['silver', 'gray', 'grey'],\n",
    "    'Blue': ['blue'],\n",
    "    'Red': ['red'],\n",
    "    'Green': ['green'],\n",
    "    'Yellow': ['yellow', 'gold'],\n",
    "    'Brown': ['brown', 'tan'],\n",
    "    'Orange': ['orange'],\n",
    "    'Purple': ['purple'],\n",
    "    'Pink': ['pink'],\n",
    "    'Tan': ['tan'],\n",
    "    'Beige': ['beige'],\n",
    "    'Metallic': ['metallic'],\n",
    "    'Clearcoat': ['clearcoat']\n",
    "}\n",
    "\n",
    "def map_color(color, color_groups):\n",
    "    color = color.lower()  \n",
    "    for standard_color, keywords in color_groups.items():\n",
    "        if any(keyword in color for keyword in keywords):\n",
    "            return standard_color\n",
    "    return 'Unknown' \n",
    "\n",
    "## Apply the mapping function to standardize colors\n",
    "training['VehColorExt_Standardized'] = training['VehColorExt'].apply(map_color, color_groups=color_groups)\n",
    "training['VehColorInt_Standardized'] = training['VehColorInt'].apply(map_color, color_groups=color_groups)\n",
    "\n",
    "## Convert to dummies\n",
    "training = pd.get_dummies(training, columns=['VehColorExt_Standardized'])\n",
    "training = pd.get_dummies(training, columns=['VehColorInt_Standardized'])\n",
    "\n",
    "## Drop redundant columns\n",
    "training = training.drop(columns=['VehColorExt'])\n",
    "training = training.drop(columns=['VehColorInt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "895bbfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vehicle Drive Train\n",
    "training['VehDriveTrain'].value_counts()\n",
    "\n",
    "training['VehDriveTrain'].fillna('Unknown', inplace=True)\n",
    "\n",
    "## keywords for drive type groups\n",
    "drive_type_groups = {\n",
    "    '4WD': ['4wd', '4x4', 'four wheel drive'],\n",
    "    'FWD': ['fwd', 'front wheel drive', 'front-wheel drive'],\n",
    "    'AWD': ['awd', 'all wheel drive', 'all-wheel drive', 'allwheeldrive'],\n",
    "    '2WD': ['2wd']\n",
    "}\n",
    "\n",
    "def map_drive_type(drive_type, drive_type_groups):\n",
    "    drive_type = drive_type.lower()  # Convert to lower case for easier matching\n",
    "    for standard_drive_type, keywords in drive_type_groups.items():\n",
    "        if any(keyword in drive_type for keyword in keywords):\n",
    "            return standard_drive_type\n",
    "    return 'Unknown'  \n",
    "\n",
    "## Apply the mapping function to standardize drive types\n",
    "training['DriveType_Standardized'] = training['VehDriveTrain'].apply(map_drive_type, drive_type_groups=drive_type_groups)\n",
    "\n",
    "## Convert to dummies\n",
    "training = pd.get_dummies(training, columns=['DriveType_Standardized'])\n",
    "\n",
    "## Drop redundant columns\n",
    "training = training.drop(columns=['VehDriveTrain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "a831dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##VehEngine\n",
    "training['VehEngine'].fillna('Unknown', inplace=True)\n",
    "\n",
    "## Define keywords for engine groups\n",
    "engine_groups = {\n",
    "    '3.6L V6': ['3.6l v6'],\n",
    "    '3.0L V6': ['3.0l v6'],\n",
    "    'Regular Unleaded V-6': ['regular unleaded v-6'],\n",
    "    'Gas V6': ['gas v6'],\n",
    "    'V6 Natural Aspiration': ['v6 natural aspiration'],\n",
    "    'V6 Flex Fuel': ['flex fuel', 'v6, flex fuel'],\n",
    "    '6 Cylinder': ['6 cylinder'],\n",
    "    '5.7L': ['5.7l']\n",
    "}\n",
    "\n",
    "def map_engine(engine, engine_groups):\n",
    "    engine = engine.lower()  \n",
    "    for standard_engine, keywords in engine_groups.items():\n",
    "        if any(keyword in engine for keyword in keywords):\n",
    "            return standard_engine\n",
    "    return 'Unknown'  \n",
    "\n",
    "## Apply the mapping function to standardize engines\n",
    "training['VehEngine_Standardized'] = training['VehEngine'].apply(map_engine, engine_groups=engine_groups)\n",
    "\n",
    "## Dummies\n",
    "training = pd.get_dummies(training, columns=['VehEngine_Standardized'])\n",
    "\n",
    "## Drop redundant columns\n",
    "training = training.drop(columns=['VehEngine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "288c9759",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Veh Feats\n",
    "## Most data covered in the other categories already drop\n",
    "training = training.drop(columns=['VehFeats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "aad19059",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VehFuel\n",
    "training['VehFuel'].fillna('Unknown', inplace=True)\n",
    "training = pd.get_dummies(training, columns=['VehFuel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "1db32e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VehHistory \n",
    "## Condense to accidents or not\n",
    "training['VehHistory'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Function to categorize based on \"Accident\"\n",
    "def categorize_accident(status):\n",
    "    if 'accident' in status.lower():\n",
    "        return 'Accident'\n",
    "    else:\n",
    "        return 'No Accident'\n",
    "\n",
    "## Apply the function to create a new column\n",
    "training['AccidentCategory'] = training['VehHistory'].apply(categorize_accident)\n",
    "\n",
    "training = pd.get_dummies(training, columns=['AccidentCategory'])\n",
    "\n",
    "## Drop redundant columns\n",
    "training = training.drop(columns=['VehHistory'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "39f680fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VehListDays\n",
    "training['VehListdays'].fillna(training['VehListdays'].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "id": "d6a0158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##VehMake\n",
    "training['VehMake'].fillna('Unknown', inplace=True)\n",
    "training = pd.get_dummies(training, columns=['VehMake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "de4610fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##VehMileage\n",
    "training['VehMileage'].fillna(training['VehMileage'].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "id": "f73ba0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VehModel\n",
    "## Same As vehicle make \n",
    "training = training.drop(columns=['VehModel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "fece092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VehPriceLabel\n",
    "## This could implement bias going to remove\n",
    "training = training.drop(columns=['VehPriceLabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "3d1c4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VehSellerNotes\n",
    "## Most information is already contained in the other features\n",
    "training = training.drop(columns=['VehSellerNotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "id": "85e8c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VehType\n",
    "## Every car is used can remove\n",
    "training = training.drop(columns=['VehType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "9e4d394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##VehTransmision\n",
    "## Mostly everything is automatic can remove as well\n",
    "training = training.drop(columns=['VehTransmission'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "b836ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VehYear\n",
    "## Convert to dummies\n",
    "training = pd.get_dummies(training, columns=['VehYear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "c4f08b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ListingID = training['ListingID']\n",
    "VehTrim = training['Vehicle_Trim']\n",
    "Price = training['Dealer_Listing_Price']\n",
    "training = training.drop(columns=['ListingID', 'Vehicle_Trim', 'Dealer_Listing_Price'])\n",
    "\n",
    "## Remove columns not in test data\n",
    "train_columns = ['SellerState_HI', 'VehColorInt_Standardized_Blue', 'SellerState_OR', 'SellerState_VT', \n",
    "                 'VehColorExt_Standardized_Purple', 'VehColorExt_Standardized_Yellow', 'SellerState_ME', \n",
    "                 'VehColorExt_Standardized_Pink', 'SellerState_MT', 'VehFuel_Unknown', 'SellerState_NM', \n",
    "                 'DriveType_Standardized_2WD']\n",
    "\n",
    "training = training.drop(columns = train_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "2c48e5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 3845.2123946382235\n"
     ]
    }
   ],
   "source": [
    "#Modeling Price\n",
    "## Convert all boolean columns to integers\n",
    "bool_cols = training.select_dtypes(include=['bool']).columns\n",
    "training[bool_cols] = training[bool_cols].astype(int)\n",
    "\n",
    "## Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(training, Price, test_size=0.2, random_state=42)\n",
    "\n",
    "## Convert data to DMatrix format for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "\n",
    "## Define parameters for the XGBoost model\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "## Train the model\n",
    "num_round = 100\n",
    "bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "## Make predictions\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "## Evaluate the model\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "rmse = mse ** 0.5\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "b4ad83d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5560307955517536\n"
     ]
    }
   ],
   "source": [
    "#Modeling Trim\n",
    "categories = {\n",
    "    'Limited': 0,\n",
    "    'Premium Luxury': 1,\n",
    "    'Laredo': 2,\n",
    "    'Luxury': 3,\n",
    "    'Overland': 4,\n",
    "    'Altitude': 5,\n",
    "    'Summit': 6,\n",
    "    'Trailhawk': 7,\n",
    "    'Base': 8,\n",
    "    'Platinum': 9,\n",
    "    'High Altitude': 10,\n",
    "    'SRT': 11,\n",
    "    'FWD': 12,\n",
    "    'Luxury FWD': 13,\n",
    "    'Laredo E': 14,\n",
    "    'Premium Luxury FWD': 15,\n",
    "    'Trackhawk': 16,\n",
    "    'Sterling Edition': 17,\n",
    "    'Luxury AWD': 18,\n",
    "    'Platinum AWD': 19,\n",
    "    'Premium Luxury AWD': 20,\n",
    "    '75th Anniversary': 21,\n",
    "    'Limited 75th Anniversary Edition': 22,\n",
    "    'SRT Night': 23,\n",
    "    'Upland': 24,\n",
    "    'Limited 4x4': 25,\n",
    "    '75th Anniversary Edition': 26,\n",
    "    'Limited 75th Anniversary': 27\n",
    "}\n",
    "\n",
    "## Apply mapping\n",
    "training['Trim'] = VehTrim.map(categories)\n",
    "training['Trim'].fillna(0, inplace=True)\n",
    "\n",
    "## Define features and target\n",
    "y = training['Trim']\n",
    "X = training.drop('Trim', axis=1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svm_classifier = SVC(kernel='rbf', decision_function_shape='ovr', random_state=42)\n",
    "\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "##Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "119cf374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all to test data\n",
    "test = pd.read_csv('Test_DataSet.csv')\n",
    "\n",
    "top_16_us_cities_by_population = [\n",
    "    \"New York City\",\n",
    "    \"Los Angeles\",\n",
    "    \"Chicago\",\n",
    "    \"Houston\",\n",
    "    \"Phoenix\",\n",
    "    \"Philadelphia\",\n",
    "    \"San Antonio\",\n",
    "    \"San Diego\",\n",
    "    \"Dallas\",\n",
    "    \"San Jose\",\n",
    "    \"Austin\",\n",
    "    \"Jacksonville\",\n",
    "    \"Fort Worth\",\n",
    "    \"Columbus\",\n",
    "    \"Charlotte\",\n",
    "    \"Indianapolis\"\n",
    "]\n",
    "test['high_population'] = test['SellerCity'].apply(lambda x: 1 if x in top_16_us_cities_by_population else 0)\n",
    "\n",
    "##Dealing with seller information\n",
    "## We will only keep state and a high low city population indicator to avoid overfitting\n",
    "test = test.drop(columns=['SellerCity', 'SellerZip'])\n",
    "\n",
    "##Convert states into booleans which will allow XGboost to work with these features.\n",
    "test = pd.get_dummies(test, columns=['SellerState'])\n",
    "\n",
    "## Check SellerListSrc for NA and replace values then convert to dummy variables\n",
    "test.loc[test['SellerListSrc'].isna()]\n",
    "test['SellerListSrc'] = test['SellerListSrc'].fillna('Sell It Yourself')\n",
    "test = pd.get_dummies(test, columns=['SellerListSrc'])\n",
    "\n",
    "## SellerIsPriv\n",
    "## Only 14 Trues this will not provide that much information and potentially cause overfitting\n",
    "## remove this column \n",
    "test = test.drop(columns=['SellerIsPriv'])\n",
    "\n",
    "## SellerName\n",
    "## Most of the names have the type of the car in it\n",
    "## We can delete this as this information is contained in VehMake\n",
    "test = test.drop(columns=['SellerName'])\n",
    "\n",
    "##Vehicle Information\n",
    "## VehBodyStyle \n",
    "## only SUV will not play a factor remove\n",
    "test = test.drop(columns=['VehBodystyle'])\n",
    "\n",
    "\n",
    "test['VehColorExt'].fillna('Unknown', inplace=True)\n",
    "test['VehColorInt'].fillna('Unknown', inplace=True)\n",
    "\n",
    "##Convert to simpler color groups\n",
    "color_groups = {\n",
    "    'Black': ['black'],\n",
    "    'White': ['white'],\n",
    "    'Silver': ['silver', 'gray', 'grey'],\n",
    "    'Blue': ['blue'],\n",
    "    'Red': ['red'],\n",
    "    'Green': ['green'],\n",
    "    'Yellow': ['yellow', 'gold'],\n",
    "    'Brown': ['brown', 'tan'],\n",
    "    'Orange': ['orange'],\n",
    "    'Purple': ['purple'],\n",
    "    'Pink': ['pink'],\n",
    "    'Tan': ['tan'],\n",
    "    'Beige': ['beige'],\n",
    "    'Metallic': ['metallic'],\n",
    "    'Clearcoat': ['clearcoat']\n",
    "}\n",
    "\n",
    "def map_color(color, color_groups):\n",
    "    color = color.lower()  \n",
    "    for standard_color, keywords in color_groups.items():\n",
    "        if any(keyword in color for keyword in keywords):\n",
    "            return standard_color\n",
    "    return 'Unknown' \n",
    "\n",
    "## Apply the mapping function to standardize colors\n",
    "test['VehColorExt_Standardized'] = test['VehColorExt'].apply(map_color, color_groups=color_groups)\n",
    "test['VehColorInt_Standardized'] = test['VehColorInt'].apply(map_color, color_groups=color_groups)\n",
    "\n",
    "##Convert to dummies\n",
    "test = pd.get_dummies(test, columns=['VehColorExt_Standardized'])\n",
    "test = pd.get_dummies(test, columns=['VehColorInt_Standardized'])\n",
    "\n",
    "##Drop redundant columns\n",
    "test = test.drop(columns=['VehColorExt'])\n",
    "test = test.drop(columns=['VehColorInt'])\n",
    "\n",
    "## Vehicle Drive Train\n",
    "test['VehDriveTrain'].fillna('Unknown', inplace=True)\n",
    "\n",
    "##keywords for drive type groups\n",
    "drive_type_groups = {\n",
    "    '4WD': ['4wd', '4x4', 'four wheel drive'],\n",
    "    'FWD': ['fwd', 'front wheel drive', 'front-wheel drive'],\n",
    "    'AWD': ['awd', 'all wheel drive', 'all-wheel drive', 'allwheeldrive'],\n",
    "    '2WD': ['2wd']\n",
    "}\n",
    "\n",
    "def map_drive_type(drive_type, drive_type_groups):\n",
    "    drive_type = drive_type.lower()  # Convert to lower case for easier matching\n",
    "    for standard_drive_type, keywords in drive_type_groups.items():\n",
    "        if any(keyword in drive_type for keyword in keywords):\n",
    "            return standard_drive_type\n",
    "    return 'Unknown'  \n",
    "\n",
    "## Apply the mapping function to standardize drive types\n",
    "test['DriveType_Standardized'] = test['VehDriveTrain'].apply(map_drive_type, drive_type_groups=drive_type_groups)\n",
    "\n",
    "##Convert to dummies\n",
    "test = pd.get_dummies(test, columns=['DriveType_Standardized'])\n",
    "\n",
    "##Drop redundant columns\n",
    "test = test.drop(columns=['VehDriveTrain'])\n",
    "\n",
    "##VehEngine\n",
    "test['VehEngine'].fillna('Unknown', inplace=True)\n",
    "\n",
    "## Define keywords for engine groups\n",
    "engine_groups = {\n",
    "    '3.6L V6': ['3.6l v6'],\n",
    "    '3.0L V6': ['3.0l v6'],\n",
    "    'Regular Unleaded V-6': ['regular unleaded v-6'],\n",
    "    'Gas V6': ['gas v6'],\n",
    "    'V6 Natural Aspiration': ['v6 natural aspiration'],\n",
    "    'V6 Flex Fuel': ['flex fuel', 'v6, flex fuel'],\n",
    "    '6 Cylinder': ['6 cylinder'],\n",
    "    '5.7L': ['5.7l']\n",
    "}\n",
    "\n",
    "def map_engine(engine, engine_groups):\n",
    "    engine = engine.lower()  \n",
    "    for standard_engine, keywords in engine_groups.items():\n",
    "        if any(keyword in engine for keyword in keywords):\n",
    "            return standard_engine\n",
    "    return 'Unknown'  # If no keyword matches, categorize as 'Other'\n",
    "\n",
    "## Apply the mapping function to standardize engines\n",
    "test['VehEngine_Standardized'] = test['VehEngine'].apply(map_engine, engine_groups=engine_groups)\n",
    "\n",
    "## Dummies\n",
    "test = pd.get_dummies(test, columns=['VehEngine_Standardized'])\n",
    "\n",
    "##Drop redundant columns\n",
    "test = test.drop(columns=['VehEngine'])\n",
    "\n",
    "## Veh Feats\n",
    "## Most data covered in the other categories already drop\n",
    "test = test.drop(columns=['VehFeats'])\n",
    "\n",
    "## VehFuel\n",
    "test['VehFuel'].fillna('Unknown', inplace=True)\n",
    "test = pd.get_dummies(test, columns=['VehFuel'])\n",
    "\n",
    "## VehHistory \n",
    "## Condense to accidents or not\n",
    "test['VehHistory'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Function to categorize based on \"Accident\"\n",
    "def categorize_accident(status):\n",
    "    if 'accident' in status.lower():\n",
    "        return 'Accident'\n",
    "    else:\n",
    "        return 'No Accident'\n",
    "\n",
    "## Apply the function to create a new column\n",
    "test['AccidentCategory'] = test['VehHistory'].apply(categorize_accident)\n",
    "\n",
    "test = pd.get_dummies(test, columns=['AccidentCategory'])\n",
    "\n",
    "##Drop redundant columns\n",
    "test = test.drop(columns=['VehHistory'])\n",
    "\n",
    "## VehListDays\n",
    "test['VehListdays'].fillna(test['VehListdays'].median(), inplace = True)\n",
    "\n",
    "##VehMake\n",
    "test['VehMake'].fillna('Unknown', inplace=True)\n",
    "test = pd.get_dummies(test, columns=['VehMake'])\n",
    "\n",
    "##VehMileage\n",
    "test['VehMileage'].fillna(test['VehMileage'].median(), inplace = True)\n",
    "\n",
    "## VehModel\n",
    "## Same As vehicle make \n",
    "test = test.drop(columns=['VehModel'])\n",
    "\n",
    "## VehPriceLabel\n",
    "## This could implement bias going to remove\n",
    "test = test.drop(columns=['VehPriceLabel'])\n",
    "\n",
    "## VehSellerNotes\n",
    "## Most information is already contained in the other features\n",
    "test = test.drop(columns=['VehSellerNotes'])\n",
    "\n",
    "## VehType\n",
    "## Every car is used can remove\n",
    "test = test.drop(columns=['VehType'])\n",
    "\n",
    "##VehTransmision\n",
    "## Mostly everything is automatic can remove as well\n",
    "test = test.drop(columns=['VehTransmission'])\n",
    "\n",
    "## VehYear\n",
    "## Convert to dummies\n",
    "test = pd.get_dummies(test, columns=['VehYear'])\n",
    "\n",
    "test = test.drop(columns=['SellerListSrc_Cadillac Certified Program'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "id": "3fbeaf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions\n",
    "ListingID_test = test['ListingID']\n",
    "test = test.drop(columns=['ListingID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "f2393234",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15024\\976576908.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdnewtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "# Price Predictions\n",
    "## Make predictions\n",
    "dnewtest = xgb.DMatrix(test)\n",
    "preds = bst.predict(dnewtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "59bee99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category Predictions\n",
    "X_test_scaled = scaler.transform(test)\n",
    "y_pred = svm_classifier.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "cd59614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answers\n",
    "ANS = pd.DataFrame({\n",
    "    'ListingID': ListingID_test,\n",
    "    'Trim': y_pred,\n",
    "    'Price': preds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "40e1578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Change Answers to string\n",
    "categories = {\n",
    "    'Limited': 0,\n",
    "    'Premium Luxury': 1,\n",
    "    'Laredo': 2,\n",
    "    'Luxury': 3,\n",
    "    'Overland': 4,\n",
    "    'Altitude': 5,\n",
    "    'Summit': 6,\n",
    "    'Trailhawk': 7,\n",
    "    'Base': 8,\n",
    "    'Platinum': 9,\n",
    "    'High Altitude': 10,\n",
    "    'SRT': 11,\n",
    "    'FWD': 12,\n",
    "    'Luxury FWD': 13,\n",
    "    'Laredo E': 14,\n",
    "    'Premium Luxury FWD': 15,\n",
    "    'Trackhawk': 16,\n",
    "    'Sterling Edition': 17,\n",
    "    'Luxury AWD': 18,\n",
    "    'Platinum AWD': 19,\n",
    "    'Premium Luxury AWD': 20,\n",
    "    '75th Anniversary': 21,\n",
    "    'Limited 75th Anniversary Edition': 22,\n",
    "    'SRT Night': 23,\n",
    "    'Upland': 24,\n",
    "    'Limited 4x4': 25,\n",
    "    '75th Anniversary Edition': 26,\n",
    "    'Limited 75th Anniversary': 27\n",
    "}\n",
    "\n",
    "## Create the inverse mapping\n",
    "inverse_categories = {v: k for k, v in categories.items()}\n",
    "\n",
    "ANS['Trim'] = training['Trim'].map(inverse_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "f57cda13",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save predictions\n",
    "file_path = r'C:\\Users\\samsn\\OneDrive\\Boeing\\Boeing Data Science Challenge Problem\\output.csv'\n",
    "\n",
    "ANS.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "291379e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ListingID  1000 non-null   int64  \n",
      " 1   Trim       1000 non-null   object \n",
      " 2   Price      1000 non-null   float32\n",
      "dtypes: float32(1), int64(1), object(1)\n",
      "memory usage: 19.7+ KB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e651b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
